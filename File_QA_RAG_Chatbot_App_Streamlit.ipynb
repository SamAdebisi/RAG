{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Implement an advanced RAG System with ChatGPT, LangChain and Streamlit to build a File QA UI-based chatbot with the following features:\n",
    "\n",
    "- PDF Document Upload and Indexing\n",
    "- RAG System for query analysis and response\n",
    "- Result streaming capabilities (Real-time output)\n",
    "- Show document sources of the answer from RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.3.11\n",
    "!pip install langchain-openai==0.2.12\n",
    "!pip install langchain-community==0.3.11\n",
    "!pip install streamlit==1.32.2\n",
    "!pip install pyngrok==7.2.2\n",
    "!pip install PyMuPDF==1.24.0\n",
    "!pip install chromadb==0.5.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('chatgpt_api_credentials.yml', 'r') as file:\n",
    "    api_creds = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_creds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = api_creds['openai_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain_core.callbacks.base import BaseCallbackHandler\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from operator import itemgetter\n",
    "import streamlit as st\n",
    "import tempfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Customize initial app landing page\n",
    "st.set_page_config(page_title=\"File QA Chatbot\", page_icon=\"ðŸ¤–\")\n",
    "st.title(\"Welcome to File QA RAG Chatbot ðŸ¤–\")\n",
    "\n",
    "@st.cache_resource(ttl=\"1h\")\n",
    "# Takes uploaded PDFs, creates document chunks, computes embeddings\n",
    "# Stores document chunks and embeddings in a Vector DB\n",
    "# Returns a retriever which can look up the Vector DB\n",
    "# to return documents based on user input\n",
    "# Stores this in the cache\n",
    "def configure_retriever(uploaded_files):\n",
    "  # Read documents\n",
    "  docs = []\n",
    "  temp_dir = tempfile.TemporaryDirectory()\n",
    "  for file in uploaded_files:\n",
    "    temp_filepath = os.path.join(temp_dir.name, file.name)\n",
    "    with open(temp_filepath, \"wb\") as f:\n",
    "      f.write(file.getvalue())\n",
    "    loader = PyMuPDFLoader(temp_filepath)\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "  # Split into documents chunks\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,\n",
    "                                                 chunk_overlap=200)\n",
    "  doc_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "  # Create document embeddings and store in Vector DB\n",
    "  embeddings_model = OpenAIEmbeddings()\n",
    "  vectordb = Chroma.from_documents(doc_chunks, embeddings_model)\n",
    "\n",
    "  # Define retriever object\n",
    "  retriever = vectordb.as_retriever()\n",
    "  return retriever\n",
    "\n",
    "# Manages live updates to a Streamlit app's display by appending new text tokens\n",
    "# to an existing text stream and rendering the updated text in Markdown\n",
    "class StreamHandler(BaseCallbackHandler):\n",
    "  def __init__(self, container, initial_text=\"\"):\n",
    "    self.container = container\n",
    "    self.text = initial_text\n",
    "\n",
    "  def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "    self.text += token\n",
    "    self.container.markdown(self.text)\n",
    "\n",
    "# Creates UI element to accept PDF uploads\n",
    "uploaded_files = st.sidebar.file_uploader(\n",
    "  label=\"Upload PDF files\", type=[\"pdf\"],\n",
    "  accept_multiple_files=True\n",
    ")\n",
    "if not uploaded_files:\n",
    "  st.info(\"Please upload PDF documents to continue.\")\n",
    "  st.stop()\n",
    "\n",
    "# Create retriever object based on uploaded PDFs\n",
    "retriever = configure_retriever(uploaded_files)\n",
    "\n",
    "# Load a connection to ChatGPT LLM\n",
    "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.1,\n",
    "                     streaming=True)\n",
    "\n",
    "# Create a prompt template for QA RAG System\n",
    "qa_template = \"\"\"\n",
    "              Use only the following pieces of context to answer the question at the end.\n",
    "              If you don't know the answer, just say that you don't know,\n",
    "              don't try to make up an answer. Keep the answer as concise as possible.\n",
    "\n",
    "              {context}\n",
    "\n",
    "              Question: {question}\n",
    "              \"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
    "\n",
    "# This function formats retrieved documents before sending to LLM\n",
    "def format_docs(docs):\n",
    "  return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# Create a QA RAG System Chain\n",
    "qa_rag_chain = (\n",
    "  {\n",
    "    \"context\": itemgetter(\"question\") # based on the user question get context docs\n",
    "      |\n",
    "    retriever\n",
    "      |\n",
    "    format_docs,\n",
    "    \"question\": itemgetter(\"question\") # user question\n",
    "  }\n",
    "    |\n",
    "  qa_prompt # prompt with above user question and context\n",
    "    |\n",
    "  chatgpt # above prompt is sent to the LLM for response\n",
    ")\n",
    "\n",
    "# Store conversation history in Streamlit session state\n",
    "streamlit_msg_history = StreamlitChatMessageHistory(key=\"langchain_messages\")\n",
    "\n",
    "# Shows the first message when app starts\n",
    "if len(streamlit_msg_history.messages) == 0:\n",
    "  streamlit_msg_history.add_ai_message(\"Please ask your question?\")\n",
    "\n",
    "# Render current messages from StreamlitChatMessageHistory\n",
    "for msg in streamlit_msg_history.messages:\n",
    "  st.chat_message(msg.type).write(msg.content)\n",
    "\n",
    "# Callback handler which does some post-processing on the LLM response\n",
    "# Used to post the top 3 document sources used by the LLM in RAG response\n",
    "class PostMessageHandler(BaseCallbackHandler):\n",
    "  def __init__(self, msg: st.write):\n",
    "    BaseCallbackHandler.__init__(self)\n",
    "    self.msg = msg\n",
    "    self.sources = []\n",
    "\n",
    "  def on_retriever_end(self, documents, *, run_id, parent_run_id, **kwargs):\n",
    "    source_ids = []\n",
    "    for d in documents: # retrieved documents from retriever based on user query\n",
    "      metadata = {\n",
    "        \"source\": d.metadata[\"source\"],\n",
    "        \"page\": d.metadata[\"page\"],\n",
    "        \"content\": d.page_content[:200]\n",
    "      }\n",
    "      idx = (metadata[\"source\"], metadata[\"page\"])\n",
    "      if idx not in source_ids: # store unique source documents\n",
    "        source_ids.append(idx)\n",
    "        self.sources.append(metadata)\n",
    "\n",
    "  def on_llm_end(self, response, *, run_id, parent_run_id, **kwargs):\n",
    "    if len(self.sources):\n",
    "      st.markdown(\"__Sources:__ \"+\"\\n\")\n",
    "      st.dataframe(data=pd.DataFrame(self.sources[:3]),\n",
    "                    width=1000) # Top 3 sources\n",
    "\n",
    "\n",
    "# If user inputs a new prompt, display it and show the response\n",
    "if user_prompt := st.chat_input():\n",
    "  st.chat_message(\"human\").write(user_prompt)\n",
    "  # This is where response from the LLM is shown\n",
    "  with st.chat_message(\"ai\"):\n",
    "    # Initializing an empty data stream\n",
    "    stream_handler = StreamHandler(st.empty())\n",
    "    # UI element to write RAG sources after LLM response\n",
    "    sources_container = st.write(\"\")\n",
    "    pm_handler = PostMessageHandler(sources_container)\n",
    "    config = {\"callbacks\": [stream_handler, pm_handler]}\n",
    "    # Get LLM response\n",
    "    response = qa_rag_chain.invoke({\"question\": user_prompt},\n",
    "                                    config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
